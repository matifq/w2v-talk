{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tutorial\n",
    "\n",
    "In case you missed the buzz, word2vec is a widely featured as a member of the “new wave” of machine learning algorithms based on neural networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow). Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word, with remarkable linear relationships that allow us to do things like vec(“king”) – vec(“man”) + vec(“woman”) =~ vec(“queen”), or vec(“Montreal Canadiens”) – vec(“Montreal”) + vec(“Toronto”) resembles the vector for “Toronto Maple Leafs”.\n",
    "\n",
    "Word2vec is very useful in [automatic text tagging](https://github.com/RaRe-Technologies/movie-plots-by-genre), recommender systems and machine translation.\n",
    "\n",
    "Check out an [online word2vec demo](http://radimrehurek.com/2014/02/word2vec-tutorial/#app) where you can try this vector algebra for yourself. That demo runs `word2vec` on the Google News dataset, of **about 100 billion words**.\n",
    "\n",
    "## This tutorial\n",
    "\n",
    "In this tutorial you will learn how to train and evaluate word2vec models on your business data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Input\n",
    "Starting from the beginning, gensim’s `word2vec` expects a sequence of sentences as its input. Each sentence is a list of words (utf8 strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 22.6MB 28kB/s eta 0:00:011 0% |▏                               | 122kB 2.1MB/s eta 0:00:11    1% |▋                               | 399kB 4.5MB/s eta 0:00:05    3% |█                               | 737kB 5.3MB/s eta 0:00:05    10% |███▍                            | 2.4MB 4.3MB/s eta 0:00:05    24% |███████▊                        | 5.4MB 11.0MB/s eta 0:00:02    59% |███████████████████             | 13.5MB 6.5MB/s eta 0:00:02    78% |█████████████████████████       | 17.7MB 6.9MB/s eta 0:00:01    79% |█████████████████████████▌      | 18.0MB 5.2MB/s eta 0:00:01    81% |██████████████████████████▏     | 18.4MB 9.8MB/s eta 0:00:01    86% |███████████████████████████▊    | 19.5MB 2.8MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/69/c92661a333f733510628f28b8282698b62cdead37291c8491f3271677c02/smart_open-1.5.7.tar.gz\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/b7/a88a67002b1185ed9a8e8a6ef15266728c2361fcb4f1d02ea331e4c7741d/boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 391kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/60/03977a02a8a2c0a41f1bba7178bfb334c6784d45dd849019c1c9fa4c6f92/boto3-1.7.38-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Collecting botocore<1.11.0,>=1.10.38 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/32/3656404fd2b5714192c41aa8bc35c78d0efaf3732480a40f7c56a8af8db1/botocore-1.10.38-py2.py3-none-any.whl (4.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.3MB 164kB/s eta 0:00:01   10% |███▍                            | 460kB 7.2MB/s eta 0:00:01    38% |████████████▎                   | 1.7MB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.11.0,>=1.10.38->boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.38->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/b1/9e/7d/bb3d3b55c597e72617140a0638c06382a5f17283881eae163e\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.48.0 boto3-1.7.38 botocore-1.10.38 bz2file-0.98 docutils-0.14 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.7\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:04,010 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:04,016 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:04,030 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2018-06-14 14:34:04,035 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:04,043 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-06-14 14:34:04,045 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-06-14 14:34:04,047 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-06-14 14:34:04,050 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-06-14 14:34:04,051 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-06-14 14:34:04,052 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-06-14 14:34:04,054 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:04,058 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:04,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,070 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,074 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,077 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,080 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,082 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,084 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,086 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,088 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 174 effective words/s\n",
      "2018-06-14 14:34:04,092 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,096 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,104 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,105 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 22 effective words/s\n",
      "2018-06-14 14:34:04,108 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the input as a Python built-in list is convenient, but can use up a lot of RAM when the input is large.\n",
    "\n",
    "Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some toy data to use with the following example\n",
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f1.txt', './data/f2.txt']\n",
    "\n",
    "for i, fname in enumerate(filenames):\n",
    "    with smart_open.smart_open(fname, 'w') as fout:\n",
    "        for line in sentences[i]:\n",
    "            fout.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in smart_open(os.path.join(self.dirname, fname), 'r'):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first'], ['sentence'], ['second'], ['sentence']]\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentences('./data/') # a memory-friendly iterator\n",
    "print(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:04,186 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:04,196 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:04,212 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2018-06-14 14:34:04,214 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:04,217 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-06-14 14:34:04,220 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-06-14 14:34:04,223 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-06-14 14:34:04,226 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-06-14 14:34:04,229 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-06-14 14:34:04,230 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-06-14 14:34:04,231 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:04,232 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:04,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,266 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,289 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,320 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 44 effective words/s\n",
      "2018-06-14 14:34:04,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,344 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,369 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,371 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 8 effective words/s\n",
      "2018-06-14 14:34:04,375 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# generate the Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'first': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadcba8>, 'sentence': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadc5f8>, 'second': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadcf60>}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to further preprocess the words from the files — convert to unicode, lowercase, remove numbers, extract named entities… All of this can be done inside the `MySentences` iterator and `word2vec` doesn’t need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
    "\n",
    "**Note to advanced users:** calling `Word2Vec(sentences, iter=1)` will run **two** passes over the sentences iterator. In general it runs `iter+1` passes. By the way, the default value is `iter=5` to comply with Google's word2vec in C language. \n",
    "  1. The first pass collects words and their frequencies to build an internal dictionary tree structure. \n",
    "  2. The second pass trains the neural model.\n",
    "\n",
    "These two passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:04,395 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:04,406 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:04,413 : INFO : collected 3 word types from a corpus of 4 raw words and 4 sentences\n",
      "2018-06-14 14:34:04,414 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:04,416 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2018-06-14 14:34:04,417 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2018-06-14 14:34:04,418 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2018-06-14 14:34:04,421 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2018-06-14 14:34:04,423 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2018-06-14 14:34:04,427 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2018-06-14 14:34:04,429 : INFO : resetting layer weights\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n",
      "2018-06-14 14:34:04,435 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:04,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,461 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,478 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,481 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,503 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,506 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 53 effective words/s\n",
      "2018-06-14 14:34:04,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,527 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,528 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,528 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,546 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,551 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-06-14 14:34:04,552 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 9 effective words/s\n",
      "2018-06-14 14:34:04,554 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the same model, making the 2 steps explicit\n",
    "new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training\n",
    "new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     \n",
    "new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       \n",
    "# can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=100, alpha=0.025)\n",
      "{'first': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadcba8>, 'sentence': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadc5f8>, 'second': <gensim.models.keyedvectors.Vocab object at 0x7fcfddadcf60>}\n"
     ]
    }
   ],
   "source": [
    "print(new_model)\n",
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More data would be nice\n",
    "For the following examples, we'll use the [Lee Corpus](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor) (which you already have if you've installed gensim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyText object at 0x7fcfddabac18>\n"
     ]
    }
   ],
   "source": [
    "class MyText(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower().split()\n",
    "\n",
    "sentences = MyText()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "`Word2Vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "### min_count\n",
    "`min_count` is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:04,620 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:04,623 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:04,648 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2018-06-14 14:34:04,650 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:04,662 : INFO : min_count=10 retains 806 unique words (7% of original 10186, drops 9380)\n",
      "2018-06-14 14:34:04,664 : INFO : min_count=10 leaves 40964 word corpus (68% of original 59890, drops 18926)\n",
      "2018-06-14 14:34:04,670 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2018-06-14 14:34:04,671 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2018-06-14 14:34:04,672 : INFO : downsampling leaves estimated 26224 word corpus (64.0% of prior 40964)\n",
      "2018-06-14 14:34:04,678 : INFO : estimated required memory for 806 words and 100 dimensions: 1047800 bytes\n",
      "2018-06-14 14:34:04,680 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:04,696 : INFO : training model with 3 workers on 806 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:04,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,754 : INFO : EPOCH - 1 : training on 59890 raw words (26155 effective words) took 0.1s, 480623 effective words/s\n",
      "2018-06-14 14:34:04,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,794 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,800 : INFO : EPOCH - 2 : training on 59890 raw words (26196 effective words) took 0.0s, 652145 effective words/s\n",
      "2018-06-14 14:34:04,847 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,855 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,864 : INFO : EPOCH - 3 : training on 59890 raw words (26052 effective words) took 0.1s, 434301 effective words/s\n",
      "2018-06-14 14:34:04,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,927 : INFO : EPOCH - 4 : training on 59890 raw words (26275 effective words) took 0.1s, 444911 effective words/s\n",
      "2018-06-14 14:34:04,975 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:04,976 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:04,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:04,982 : INFO : EPOCH - 5 : training on 59890 raw words (26259 effective words) took 0.1s, 501987 effective words/s\n",
      "2018-06-14 14:34:04,985 : INFO : training on a 299450 raw words (130937 effective words) took 0.3s, 456535 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of min_count=5\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### size\n",
    "`size` is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:04,992 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:04,995 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:05,028 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2018-06-14 14:34:05,031 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:05,037 : INFO : min_count=5 retains 1723 unique words (16% of original 10186, drops 8463)\n",
      "2018-06-14 14:34:05,038 : INFO : min_count=5 leaves 46858 word corpus (78% of original 59890, drops 13032)\n",
      "2018-06-14 14:34:05,045 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2018-06-14 14:34:05,048 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2018-06-14 14:34:05,049 : INFO : downsampling leaves estimated 32849 word corpus (70.1% of prior 46858)\n",
      "2018-06-14 14:34:05,057 : INFO : estimated required memory for 1723 words and 200 dimensions: 3618300 bytes\n",
      "2018-06-14 14:34:05,059 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:05,084 : INFO : training model with 3 workers on 1723 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:05,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,203 : INFO : EPOCH - 1 : training on 59890 raw words (32813 effective words) took 0.1s, 281684 effective words/s\n",
      "2018-06-14 14:34:05,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,286 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,287 : INFO : EPOCH - 2 : training on 59890 raw words (32768 effective words) took 0.1s, 424367 effective words/s\n",
      "2018-06-14 14:34:05,369 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,381 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,382 : INFO : EPOCH - 3 : training on 59890 raw words (32884 effective words) took 0.1s, 373345 effective words/s\n",
      "2018-06-14 14:34:05,444 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,446 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,463 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,465 : INFO : EPOCH - 4 : training on 59890 raw words (32769 effective words) took 0.1s, 419748 effective words/s\n",
      "2018-06-14 14:34:05,528 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,545 : INFO : EPOCH - 5 : training on 59890 raw words (32858 effective words) took 0.1s, 474912 effective words/s\n",
      "2018-06-14 14:34:05,546 : INFO : training on a 299450 raw words (164092 effective words) took 0.5s, 355891 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# default value of size=100\n",
    "model = gensim.models.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workers\n",
    "`workers`, the last of the major parameters (full list [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:05,556 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:05,559 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:05,584 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2018-06-14 14:34:05,594 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:05,605 : INFO : min_count=5 retains 1723 unique words (16% of original 10186, drops 8463)\n",
      "2018-06-14 14:34:05,610 : INFO : min_count=5 leaves 46858 word corpus (78% of original 59890, drops 13032)\n",
      "2018-06-14 14:34:05,618 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2018-06-14 14:34:05,619 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2018-06-14 14:34:05,620 : INFO : downsampling leaves estimated 32849 word corpus (70.1% of prior 46858)\n",
      "2018-06-14 14:34:05,627 : INFO : estimated required memory for 1723 words and 100 dimensions: 2239900 bytes\n",
      "2018-06-14 14:34:05,628 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:05,656 : INFO : training model with 4 workers on 1723 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:05,696 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:05,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,715 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,716 : INFO : EPOCH - 1 : training on 59890 raw words (32813 effective words) took 0.1s, 572867 effective words/s\n",
      "2018-06-14 14:34:05,755 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:05,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,779 : INFO : EPOCH - 2 : training on 59890 raw words (32839 effective words) took 0.1s, 553540 effective words/s\n",
      "2018-06-14 14:34:05,826 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:05,828 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,845 : INFO : EPOCH - 3 : training on 59890 raw words (32850 effective words) took 0.1s, 519667 effective words/s\n",
      "2018-06-14 14:34:05,907 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:05,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,923 : INFO : EPOCH - 4 : training on 59890 raw words (32827 effective words) took 0.1s, 465994 effective words/s\n",
      "2018-06-14 14:34:05,977 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:05,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:05,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:05,992 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:05,993 : INFO : EPOCH - 5 : training on 59890 raw words (32903 effective words) took 0.1s, 494164 effective words/s\n",
      "2018-06-14 14:34:05,995 : INFO : training on a 299450 raw words (164232 effective words) took 0.3s, 487540 effective words/s\n",
      "2018-06-14 14:34:05,996 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# default value of workers=3 (tutorial says 1...)\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `workers` parameter only has an effect if you have [Cython](http://cython.org/) installed. Without Cython, you’ll only be able to use one core because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) (and `word2vec` training will be [miserably slow](http://rare-technologies.com/word2vec-in-python-part-two-optimizing/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "At its core, `word2vec` model parameters are stored as matrices (NumPy arrays). Each array is **#vocabulary** (controlled by min_count parameter) times **#size** (size parameter) of floats (single precision aka 4 bytes).\n",
    "\n",
    "Three such matrices are held in RAM (work is underway to reduce that number to two, or even one). So if your input contains 100,000 unique words, and you asked for layer `size=200`, the model will require approx. `100,000*200*4*3 bytes = ~229MB`.\n",
    "\n",
    "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would take a few megabytes), but unless your words are extremely loooong strings, memory footprint will be dominated by the three matrices above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "`Word2Vec` training is an unsupervised task, there’s no good way to objectively evaluate the result. Evaluation depends on your end application.\n",
    "\n",
    "Google has released their testing set of about 20,000 syntactic and semantic test examples, following the “A is to B as C is to D” task. It is provided in the 'datasets' folder.\n",
    "\n",
    "For example a syntactic analogy of comparative type is bad:worse;good:?. There are total of 9 types of syntactic comparisons in the dataset like plural nouns and nouns of opposite meaning.\n",
    "\n",
    "The semantic questions contain five types of semantic analogies, such as capital cities (Paris:France;Tokyo:?) or family members (brother:sister;dad:?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim supports the same evaluation set, in exactly the same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.accuracy() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-06-14 14:34:06,051 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-14 14:34:06,057 : INFO : family: 0.0% (0/2)\n",
      "2018-06-14 14:34:06,077 : INFO : gram3-comparative: 0.0% (0/12)\n",
      "2018-06-14 14:34:06,088 : INFO : gram4-superlative: 0.0% (0/12)\n",
      "2018-06-14 14:34:06,103 : INFO : gram5-present-participle: 0.0% (0/20)\n",
      "2018-06-14 14:34:06,121 : INFO : gram6-nationality-adjective: 0.0% (0/20)\n",
      "2018-06-14 14:34:06,135 : INFO : gram7-past-tense: 0.0% (0/20)\n",
      "2018-06-14 14:34:06,147 : INFO : gram8-plural: 0.0% (0/12)\n",
      "2018-06-14 14:34:06,155 : INFO : total: 0.0% (0/98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],\n",
       "  'section': 'family'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER')],\n",
       "  'section': 'gram3-comparative'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],\n",
       "  'section': 'gram4-superlative'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING')],\n",
       "  'section': 'gram5-present-participle'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],\n",
       "  'section': 'gram6-nationality-adjective'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID')],\n",
       "  'section': 'gram7-past-tense'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
       "  'section': 'gram8-plural'},\n",
       " {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
       " {'correct': [],\n",
       "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HIS', 'HER', 'HE', 'SHE'),\n",
       "   ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "   ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
       "   ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "   ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
       "  'section': 'total'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy('./datasets/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `accuracy` takes an \n",
    "[optional parameter](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec.accuracy) `restrict_vocab` \n",
    "which limits which test examples are to be considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
    "\n",
    "By default it uses an academic dataset WS-353 but one can create a dataset specific to your business based on it. It contains word pairs together with human-assigned similarity judgments. It measures the relatedness or co-occurrence of two words. For example, 'coast' and 'shore' are very similar as they appear in the same context. At the same time 'clothes' and 'closet' are less similar because they are related but not interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `evaluate_word_pairs` (Method will be removed in 4.0.0, use self.wv.evaluate_word_pairs() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-06-14 14:34:06,201 : INFO : Pearson correlation coefficient against /opt/conda/lib/python3.6/site-packages/gensim/test/test_data/wordsim353.tsv: 0.0993\n",
      "2018-06-14 14:34:06,203 : INFO : Spearman rank-order correlation coefficient against /opt/conda/lib/python3.6/site-packages/gensim/test/test_data/wordsim353.tsv: 0.0623\n",
      "2018-06-14 14:34:06,204 : INFO : Pairs with unknown words ratio: 85.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.099326693481591385, 0.48801650009644326),\n",
       " SpearmanrResult(correlation=0.062269487642049343, pvalue=0.66422285754165034),\n",
       " 85.55240793201133)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, **good performance on Google's or WS-353 test set doesn’t mean word2vec will work well in your application, or vice versa**. It’s always best to evaluate directly on your intended task. For an example of how to use word2vec in a classifier pipeline, see this [tutorial](https://github.com/RaRe-Technologies/movie-plots-by-genre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading models\n",
    "You can store/load models using the standard gensim methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:06,224 : INFO : saving Word2Vec object under /tmp/tmpb8tbvqehgensim_temp, separately None\n",
      "2018-06-14 14:34:06,226 : INFO : not storing attribute vectors_norm\n",
      "2018-06-14 14:34:06,229 : INFO : not storing attribute cum_table\n",
      "2018-06-14 14:34:06,246 : INFO : saved /tmp/tmpb8tbvqehgensim_temp\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkstemp\n",
    "\n",
    "fs, temp_path = mkstemp(\"gensim_temp\")  # creates a temp file\n",
    "\n",
    "model.save(temp_path)  # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:06,260 : INFO : loading Word2Vec object from /tmp/tmpb8tbvqehgensim_temp\n",
      "2018-06-14 14:34:06,276 : INFO : loading wv recursively from /tmp/tmpb8tbvqehgensim_temp.wv.* with mmap=None\n",
      "2018-06-14 14:34:06,277 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-14 14:34:06,279 : INFO : loading vocabulary recursively from /tmp/tmpb8tbvqehgensim_temp.vocabulary.* with mmap=None\n",
      "2018-06-14 14:34:06,280 : INFO : loading trainables recursively from /tmp/tmpb8tbvqehgensim_temp.trainables.* with mmap=None\n",
      "2018-06-14 14:34:06,281 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-14 14:34:06,282 : INFO : loaded /tmp/tmpb8tbvqehgensim_temp\n"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec.load(temp_path)  # open the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which uses pickle internally, optionally `mmap`‘ing the model’s internal large NumPy matrices into virtual memory directly from disk files, for inter-process memory sharing.\n",
    "\n",
    "In addition, you can load models created by the original C tool, both using its text and binary formats:\n",
    "```\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "  # using gzipped/bz2 input works too, no need to unzip:\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online training / Resuming training\n",
    "Advanced users can load a model and continue training it with more sentences and [new vocabulary words](online_w2v_tutorial.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:06,304 : INFO : loading Word2Vec object from /tmp/tmpb8tbvqehgensim_temp\n",
      "2018-06-14 14:34:06,320 : INFO : loading wv recursively from /tmp/tmpb8tbvqehgensim_temp.wv.* with mmap=None\n",
      "2018-06-14 14:34:06,321 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-14 14:34:06,322 : INFO : loading vocabulary recursively from /tmp/tmpb8tbvqehgensim_temp.vocabulary.* with mmap=None\n",
      "2018-06-14 14:34:06,323 : INFO : loading trainables recursively from /tmp/tmpb8tbvqehgensim_temp.trainables.* with mmap=None\n",
      "2018-06-14 14:34:06,324 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-14 14:34:06,326 : INFO : loaded /tmp/tmpb8tbvqehgensim_temp\n",
      "2018-06-14 14:34:06,331 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:06,332 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:06,333 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences\n",
      "2018-06-14 14:34:06,335 : INFO : Updating model with new vocabulary\n",
      "2018-06-14 14:34:06,336 : INFO : New added 0 unique words (0% of original 13) and increased the count of 0 pre-existing words (0% of original 13)\n",
      "2018-06-14 14:34:06,337 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2018-06-14 14:34:06,342 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2018-06-14 14:34:06,347 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
      "2018-06-14 14:34:06,352 : INFO : estimated required memory for 1723 words and 100 dimensions: 2239900 bytes\n",
      "2018-06-14 14:34:06,355 : INFO : updating layer weights\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n",
      "2018-06-14 14:34:06,359 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-06-14 14:34:06,361 : INFO : training model with 4 workers on 1723 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:06,365 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:06,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:06,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:06,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:06,370 : INFO : EPOCH - 1 : training on 13 raw words (6 effective words) took 0.0s, 1137 effective words/s\n",
      "2018-06-14 14:34:06,378 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:06,379 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:06,380 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:06,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:06,383 : INFO : EPOCH - 2 : training on 13 raw words (5 effective words) took 0.0s, 868 effective words/s\n",
      "2018-06-14 14:34:06,387 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:06,388 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:06,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:06,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:06,391 : INFO : EPOCH - 3 : training on 13 raw words (5 effective words) took 0.0s, 1169 effective words/s\n",
      "2018-06-14 14:34:06,396 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:06,398 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:06,399 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:06,401 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:06,403 : INFO : EPOCH - 4 : training on 13 raw words (5 effective words) took 0.0s, 712 effective words/s\n",
      "2018-06-14 14:34:06,411 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-14 14:34:06,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:06,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:06,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:06,417 : INFO : EPOCH - 5 : training on 13 raw words (4 effective words) took 0.0s, 526 effective words/s\n",
      "2018-06-14 14:34:06,418 : INFO : training on a 65 raw words (25 effective words) took 0.1s, 442 effective words/s\n",
      "2018-06-14 14:34:06,420 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(temp_path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "# cleaning up temp\n",
    "os.close(fs)\n",
    "os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tweak the `total_words` parameter to `train()`, depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C tool, `KeyedVectors.load_word2vec_format()`. You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there.\n",
    "\n",
    "## Using the model\n",
    "`Word2Vec` supports several word similarity tasks out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-06-14 14:34:06,427 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wake', 0.995140552520752)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-06-14 14:34:06,439 : WARNING : vectors for words {'lunch', 'cat', 'input'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"input is lunch he sentence cat\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999116843916\n",
      "0.993565185153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('human', 'party'))\n",
    "print(model.similarity('tree', 'murder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the probability distribution for the center word given the context words as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('more', 0.00094383128), ('government', 0.00077048101), ('training', 0.0007607498), ('continue', 0.00075521087), ('can', 0.00074793148), ('australia', 0.00074786448), ('also', 0.00074378337), ('united', 0.0007307519), ('there', 0.00072391168), ('arafat', 0.00072192884)]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_output_word(['emergency', 'beacon', 'received']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here don't look good because the training corpus is very small. To get meaningful results one needs to train on 500k+ words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the raw output vectors in your application, you can access these either on a word-by-word basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00507236,  0.01695369, -0.04579246,  0.04015445, -0.03821801,\n",
       "       -0.00806551,  0.04910685, -0.03377812, -0.06309545,  0.03624553,\n",
       "        0.01632687,  0.00671562, -0.02742973, -0.01133134,  0.0035804 ,\n",
       "        0.01326715,  0.0168173 , -0.02178709,  0.03465009,  0.0069912 ,\n",
       "        0.02599325,  0.00419654, -0.01365976, -0.0308643 , -0.02550944,\n",
       "        0.00818279,  0.01674226, -0.03719213,  0.04906338,  0.02477256,\n",
       "        0.08780177,  0.02048765,  0.01182354,  0.04090331,  0.02517812,\n",
       "       -0.0316829 , -0.02182687, -0.06434128, -0.04797867, -0.01253023,\n",
       "       -0.02586541,  0.01935347, -0.04413845,  0.02180763, -0.07296643,\n",
       "        0.01786931, -0.050268  ,  0.00752149, -0.03242191, -0.01589916,\n",
       "       -0.01787307,  0.01419847, -0.03283262, -0.011434  , -0.05323173,\n",
       "        0.03757442, -0.02009087,  0.01703732,  0.03100888, -0.02094323,\n",
       "       -0.00066713,  0.02878382, -0.03230232,  0.02279637,  0.04172381,\n",
       "        0.01062667,  0.03786817,  0.08861102,  0.02583497,  0.05968779,\n",
       "       -0.01439567,  0.03146625,  0.00912013,  0.03572998, -0.00574414,\n",
       "       -0.02753656, -0.00610145, -0.00061603, -0.02227029,  0.0121225 ,\n",
       "       -0.02152448, -0.02178184,  0.01548935, -0.02361096, -0.03360613,\n",
       "       -0.00160461,  0.05394433, -0.01224235,  0.04874012,  0.02726664,\n",
       "        0.02417857, -0.01065161, -0.0657884 ,  0.05030263,  0.00545694,\n",
       "       -0.02129923, -0.02756492, -0.06347407,  0.06000067,  0.06951768], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['tree']  # raw NumPy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…or en-masse as a 2D NumPy matrix from `model.wv.syn0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Computation\n",
    "\n",
    "The parameter `compute_loss` can be used to toggle computation of loss while training the Word2Vec model. The computed loss is stored in the model attribute `running_training_loss` and can be retrieved using the function `get_latest_training_loss` as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 14:34:06,503 : INFO : collecting all words and their counts\n",
      "2018-06-14 14:34:06,506 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-14 14:34:06,545 : INFO : collected 10186 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2018-06-14 14:34:06,556 : INFO : Loading a fresh vocabulary\n",
      "2018-06-14 14:34:06,597 : INFO : min_count=1 retains 10186 unique words (100% of original 10186, drops 0)\n",
      "2018-06-14 14:34:06,599 : INFO : min_count=1 leaves 59890 word corpus (100% of original 59890, drops 0)\n",
      "2018-06-14 14:34:06,630 : INFO : deleting the raw counts dictionary of 10186 items\n",
      "2018-06-14 14:34:06,631 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2018-06-14 14:34:06,632 : INFO : downsampling leaves estimated 47231 word corpus (78.9% of prior 59890)\n",
      "2018-06-14 14:34:06,662 : INFO : estimated required memory for 10186 words and 100 dimensions: 13241800 bytes\n",
      "2018-06-14 14:34:06,664 : INFO : resetting layer weights\n",
      "2018-06-14 14:34:06,786 : INFO : training model with 3 workers on 10186 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-14 14:34:06,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:07,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:07,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:07,054 : INFO : EPOCH - 1 : training on 59890 raw words (47230 effective words) took 0.3s, 179227 effective words/s\n",
      "2018-06-14 14:34:07,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:07,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:07,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:07,286 : INFO : EPOCH - 2 : training on 59890 raw words (47140 effective words) took 0.2s, 214456 effective words/s\n",
      "2018-06-14 14:34:07,451 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:07,489 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:07,498 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:07,499 : INFO : EPOCH - 3 : training on 59890 raw words (47202 effective words) took 0.2s, 229436 effective words/s\n",
      "2018-06-14 14:34:07,681 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:07,697 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:07,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:07,723 : INFO : EPOCH - 4 : training on 59890 raw words (47194 effective words) took 0.2s, 223475 effective words/s\n",
      "2018-06-14 14:34:07,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-14 14:34:07,993 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-14 14:34:08,003 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-14 14:34:08,004 : INFO : EPOCH - 5 : training on 59890 raw words (47214 effective words) took 0.3s, 169739 effective words/s\n",
      "2018-06-14 14:34:08,006 : INFO : training on a 299450 raw words (235980 effective words) took 1.2s, 193666 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814180.75\n"
     ]
    }
   ],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks to see effect of training loss compuation code on training time\n",
    "\n",
    "We first download and setup the test data used for getting the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lee_background_file():\n",
    "    test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "    lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "    return lee_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor', '/home/jovyan/Notebooks/text8_1000000', '/home/jovyan/Notebooks/text8_10000000', '/home/jovyan/Notebooks/text8_50000000', '/home/jovyan/Notebooks/text8']\n"
     ]
    }
   ],
   "source": [
    "input_data_files = []\n",
    "\n",
    "def setup_input_data():\n",
    "    # check if test data already present\n",
    "    if os.path.isfile('./text8') is False:\n",
    "\n",
    "        # download and decompress 'text8' corpus\n",
    "        import zipfile\n",
    "        ! wget 'http://mattmahoney.net/dc/text8.zip'\n",
    "        ! unzip 'text8.zip'\n",
    "    \n",
    "        # create 1 MB, 10 MB and 50 MB files\n",
    "        ! head -c1000000 text8 > text8_1000000\n",
    "        ! head -c10000000 text8 > text8_10000000\n",
    "        ! head -c50000000 text8 > text8_50000000\n",
    "                \n",
    "    # add 25 KB test file\n",
    "#     input_data_files.append(os.path.join(os.getcwd(), '../../gensim/test/test_data/lee_background.cor'))\n",
    "    input_data_files.append(get_data_lee_background_file())\n",
    "\n",
    "    # add 1 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_1000000'))\n",
    "\n",
    "    # add 10 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_10000000'))\n",
    "\n",
    "    # add 50 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8_50000000'))\n",
    "\n",
    "    # add 100 MB test file\n",
    "    input_data_files.append(os.path.join(os.getcwd(), 'text8'))\n",
    "\n",
    "setup_input_data()\n",
    "print(input_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the training time taken for different combinations of input data and model training parameters like `hs` and `sg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor\n",
      "/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor\n"
     ]
    }
   ],
   "source": [
    "print(get_data_lee_background_file())\n",
    "print(input_data_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python3.6/site-packages/gensim/test/test_data/lee_background.cor', '/home/jovyan/Notebooks/text8_50000000']\n"
     ]
    }
   ],
   "source": [
    "# using 25 KB and 50 MB files only for generating o/p -> comment next line for using all 5 test files\n",
    "input_data_files = [input_data_files[0], input_data_files[-2]]\n",
    "print(input_data_files)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_time_values = []\n",
    "seed_val = 42\n",
    "sg_values = [0, 1]\n",
    "hs_values = [0, 1]\n",
    "\n",
    "for data_file in input_data_files:\n",
    "    data = gensim.models.word2vec.LineSentence(data_file) \n",
    "    for sg_val in sg_values:\n",
    "        for hs_val in hs_values:\n",
    "            for loss_flag in [True, False]:\n",
    "                time_taken_list = []\n",
    "                for i in range(3):\n",
    "                    start_time = time.time()\n",
    "                    w2v_model = gensim.models.Word2Vec(data, compute_loss=loss_flag, sg=sg_val, hs=hs_val, seed=seed_val) \n",
    "                    time_taken_list.append(time.time() - start_time)\n",
    "\n",
    "                time_taken_list = np.array(time_taken_list)\n",
    "                time_mean = np.mean(time_taken_list)\n",
    "                time_std = np.std(time_taken_list)\n",
    "                train_time_values.append({'train_data': data_file, 'compute_loss': loss_flag, 'sg': sg_val, 'hs': hs_val, 'mean': time_mean, 'std': time_std})\n",
    "\n",
    "train_times_table = pd.DataFrame(train_time_values)\n",
    "train_times_table = train_times_table.sort_values(by=['train_data', 'sg', 'hs', 'compute_loss'], ascending=[False, False, True, False])\n",
    "print(train_times_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Word2Vec \"model to dict\" method to production pipeline\n",
    "Suppose, we still want more performance improvement in production. \n",
    "One good way is to cache all the similar words in a dictionary.\n",
    "So that next time when we get the similar query word, we'll search it first in the dict.\n",
    "And if it's a hit then we will show the result directly from the dictionary.\n",
    "otherwise we will query the word and then cache it so that it doesn't miss next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "logging.getLogger().setLevel(level=logging.INFO)\n",
    "\n",
    "# logging.getLogger().setLevel(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similars_precalc = {word : model.wv.most_similar(word) for word in model.wv.index2word}\n",
    "for i, (key, value) in enumerate(most_similars_precalc.items()):\n",
    "    if i==3:\n",
    "        break\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with and without caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for time being lets take 4 words randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "words = ['voted','few','their','around']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for word in words:\n",
    "    result = model.wv.most_similar(word)\n",
    "    print(result)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for word in words:\n",
    "    if 'voted' in most_similars_precalc:\n",
    "        result = most_similars_precalc[word]\n",
    "        print(result)\n",
    "    else:\n",
    "        result = model.wv.most_similar(word)\n",
    "        most_similars_precalc[word] = result\n",
    "        print(result)\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly you can see the improvement but this difference will be even larger when we take more words in the consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embeddings made by the model can be visualised by reducing dimensionality of the words to 2 dimensions using tSNE.\n",
    "\n",
    "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
    "\n",
    "Example: Semantic- words like cat, dog, cow, etc. have a tendency to lie close by\n",
    "         Syntactic- words like run, running or cut, cutting lie close together.\n",
    "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
    "\n",
    "Additional dependencies : \n",
    "- sklearn\n",
    "- numpy\n",
    "- plotly\n",
    "\n",
    "The function below can be used to plot the embeddings in an ipython notebook.\n",
    "It requires the model as the necessary parameter. If you don't have the model, you can load it by\n",
    "\n",
    "`model = gensim.models.Word2Vec.load('path/to/model')`\n",
    "\n",
    "If you don't want to plot inside a notebook, set the `plot_in_notebook` parameter to `False`.\n",
    "\n",
    "Note: the model used for the visualisation is trained on a small corpus. Thus some of the relations might not be so clear\n",
    "\n",
    "Beware : This sort dimension reduction comes at the cost of loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def reduce_dimensions(model, plot_in_notebook = True):\n",
    "\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = []        # positions in vector space\n",
    "    labels = []         # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    logging.info('starting tSNE dimensionality reduction. This may take some time.')\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "        \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        mode='text',\n",
    "        text=labels\n",
    "        )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    logging.info('All done. Plotting.')\n",
    "    \n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dimensions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we learned how to train word2vec models on your custom data and also how to evaluate it. Hope that you too will find this popular tool useful in your Machine Learning tasks!\n",
    "\n",
    "## Links\n",
    "\n",
    "\n",
    "Full `word2vec` API docs [here](http://radimrehurek.com/gensim/models/word2vec.html); get [gensim](http://radimrehurek.com/gensim/) here. Original C toolkit and `word2vec` papers by Google [here](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
