{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 844kB/s ta 0:00:01    85% |███████████████████████████▎    | 256kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras)\n",
      "Collecting keras-applications==1.0.2 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras)\n",
      "Collecting keras-preprocessing==1.0.1 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 49.1MB 10kB/s eta 0:00:0110% |                                | 81kB 1.2MB/s eta 0:00:41    3% |█▏                              | 1.7MB 1.3MB/s eta 0:00:36    13% |████▍                           | 6.8MB 2.9MB/s eta 0:00:15    16% |█████▎                          | 8.0MB 1.1MB/s eta 0:00:38    18% |██████                          | 9.3MB 3.4MB/s eta 0:00:12    24% |████████                        | 12.2MB 2.6MB/s eta 0:00:15    28% |█████████                       | 13.9MB 1.8MB/s eta 0:00:20    28% |█████████▏                      | 14.1MB 2.6MB/s eta 0:00:14    29% |█████████▌                      | 14.5MB 1.8MB/s eta 0:00:20    29% |█████████▌                      | 14.6MB 2.7MB/s eta 0:00:13    31% |██████████▎                     | 15.7MB 3.3MB/s eta 0:00:11    34% |███████████                     | 16.9MB 3.5MB/s eta 0:00:10    35% |███████████▎                    | 17.2MB 3.5MB/s eta 0:00:10    35% |███████████▍                    | 17.4MB 3.1MB/s eta 0:00:11    36% |███████████▋                    | 17.8MB 3.4MB/s eta 0:00:10    36% |███████████▉                    | 18.1MB 2.6MB/s eta 0:00:12    38% |████████████▌                   | 19.1MB 432kB/s eta 0:01:10    42% |█████████████▌                  | 20.8MB 2.9MB/s eta 0:00:10    43% |██████████████                  | 21.4MB 2.9MB/s eta 0:00:10    45% |██████████████▊                 | 22.5MB 3.0MB/s eta 0:00:09    52% |████████████████▊               | 25.7MB 1.7MB/s eta 0:00:15    54% |█████████████████▍              | 26.7MB 1.9MB/s eta 0:00:12    62% |████████████████████            | 30.6MB 1.4MB/s eta 0:00:14    63% |████████████████████▎           | 31.1MB 595kB/s eta 0:00:31    64% |████████████████████▌           | 31.4MB 394kB/s eta 0:00:45    64% |████████████████████▋           | 31.6MB 497kB/s eta 0:00:36    65% |████████████████████▉           | 32.0MB 588kB/s eta 0:00:30    65% |█████████████████████           | 32.2MB 220kB/s eta 0:01:17    74% |████████████████████████        | 36.8MB 3.0MB/s eta 0:00:05    78% |█████████████████████████       | 38.5MB 1.6MB/s eta 0:00:07    80% |█████████████████████████▊      | 39.4MB 3.4MB/s eta 0:00:03    80% |█████████████████████████▉      | 39.6MB 2.3MB/s eta 0:00:05    84% |███████████████████████████▏    | 41.7MB 3.3MB/s eta 0:00:03    85% |███████████████████████████▌    | 42.1MB 1.7MB/s eta 0:00:05    89% |████████████████████████████▌   | 43.8MB 4.7MB/s eta 0:00:02    96% |██████████████████████████████▉ | 47.3MB 2.8MB/s eta 0:00:01    97% |███████████████████████████████ | 47.6MB 2.0MB/s eta 0:00:01    98% |███████████████████████████████▋| 48.4MB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/ea/664c589ec41b9e9ac6e20cc1fe9016f3913332d0dc5498a5d7771e2835af/grpcio-1.12.1-cp36-cp36m-manylinux1_x86_64.whl (9.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.0MB 73kB/s  eta 0:00:01   18% |██████                          | 1.7MB 2.7MB/s eta 0:00:03    23% |███████▋                        | 2.1MB 3.1MB/s eta 0:00:03    26% |████████▍                       | 2.4MB 4.2MB/s eta 0:00:02    28% |█████████                       | 2.5MB 4.1MB/s eta 0:00:02    35% |███████████▎                    | 3.2MB 4.4MB/s eta 0:00:02    45% |██████████████▌                 | 4.1MB 4.5MB/s eta 0:00:02    56% |██████████████████              | 5.1MB 1.1MB/s eta 0:00:04    65% |█████████████████████           | 5.9MB 6.3MB/s eta 0:00:01    74% |████████████████████████        | 6.7MB 1.5MB/s eta 0:00:02    99% |███████████████████████████████▊| 8.9MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 208kB/s ta 0:00:011    26% |████████▌                       | 829kB 4.4MB/s eta 0:00:01    52% |████████████████▉               | 1.6MB 4.2MB/s eta 0:00:01    81% |██████████████████████████      | 2.5MB 5.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/57/8d/6664518f9b6ced0aa41cf50b989740909261d4c212557400c48e5cda0804/absl-py-0.2.2.tar.gz (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting numpy>=1.13.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 48kB/s eta 0:00:011  17% |█████▌                          | 2.1MB 1.6MB/s eta 0:00:07    55% |█████████████████▊              | 6.7MB 1.6MB/s eta 0:00:04    61% |███████████████████▉            | 7.5MB 4.6MB/s eta 0:00:02    62% |████████████████████▏           | 7.7MB 2.3MB/s eta 0:00:02    76% |████████████████████████▎       | 9.2MB 3.8MB/s eta 0:00:01    83% |██████████████████████████▋     | 10.1MB 24.0MB/s eta 0:00:01    87% |████████████████████████████    | 10.6MB 1.2MB/s eta 0:00:02    93% |█████████████████████████████▉  | 11.4MB 2.5MB/s eta 0:00:01    98% |███████████████████████████████▋| 12.0MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 791kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gast, absl-py, termcolor, html5lib\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/f8/e9/1933dbb3447ea6ef57062fd5461cb118deb8c2ed074e8344bf\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built gast absl-py termcolor html5lib\n",
      "Installing collected packages: gast, grpcio, numpy, html5lib, bleach, werkzeug, markdown, tensorboard, astor, absl-py, termcolor, tensorflow\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed absl-py-0.2.2 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.12.1 html5lib-0.9999999 markdown-2.6.11 numpy-1.14.5 tensorboard-1.8.0 tensorflow-1.8.0 termcolor-1.1.0 werkzeug-0.14.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.gutenberg.org/files/11/11-0.txt\n",
      "180224/173595 [===============================] - 1s 4us/step\n"
     ]
    }
   ],
   "source": [
    "path=get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "corpus = open(path).readlines()[:300]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.embeddings.Embedding at 0x7f6d5f09cba8>,\n",
       " <keras.layers.core.Lambda at 0x7f6da0203a90>,\n",
       " <keras.layers.core.Dense at 0x7f6da0203b38>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[348, 349, 65, 115, 10, 116, 57, 196, 197],\n",
       " [22, 117, 66, 17, 1, 67, 8, 350, 351, 29, 38, 352, 4, 27],\n",
       " [146, 38, 353, 354, 16, 355, 356, 5, 198, 5, 147, 32],\n",
       " [357, 67, 5, 118, 1, 358, 8, 1, 199, 148, 359, 360],\n",
       " [27, 22, 117, 32, 361, 29, 362, 148, 363]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "0 17412.564601898193\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "1 16152.262689352036\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "2 16024.16857266426\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "3 15933.23012804985\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "4 15825.284683167934\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "5 15722.586059451103\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "6 15632.517758071423\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "7 15551.37086135149\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "8 15473.514118924737\n",
      "  0 [[  0   0 349  65]] (1, 765)\n",
      "  1 [[  0 348  65 115]] (1, 765)\n",
      "  2 [[348 349 115  10]] (1, 765)\n",
      "  3 [[349  65  10 116]] (1, 765)\n",
      "2786\n",
      "9 15396.296807810664\n"
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for i, (x, y) in enumerate(generate_data(corpus, window_size, V)):\n",
    "        if i<4:\n",
    "            print(' ', i, x, y.shape)\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(i)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors-cbow.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors-cbow.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0.6486507654190063),\n",
       " ('one', 0.6460806131362915),\n",
       " ('this', 0.597328782081604),\n",
       " ('adventures', 0.559715211391449),\n",
       " ('alice’s', 0.5597040057182312),\n",
       " ('any', 0.5511570572853088),\n",
       " ('those', 0.5313930511474609),\n",
       " ('miles', 0.5266964435577393),\n",
       " ('help', 0.524553120136261),\n",
       " ('no', 0.5238304138183594)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 0.6103900074958801),\n",
       " ('you', 0.6003938317298889),\n",
       " ('poor', 0.5684895515441895),\n",
       " ('them', 0.5613431334495544),\n",
       " ('that', 0.5531876087188721),\n",
       " ('eat', 0.5528088212013245),\n",
       " ('now', 0.5517069697380066),\n",
       " ('dark', 0.548737108707428),\n",
       " ('marked', 0.5480719804763794),\n",
       " ('thought', 0.5456030368804932)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
